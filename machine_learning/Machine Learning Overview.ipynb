{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Overview\n",
    "This notebook gives a general overview of machine learning as it pertains to Data Science.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Machine Learning?\n",
    "Broadly speaking, Machine Learining (or ML) is a subset of AI which provides machines the abliity to learn automatically & improve from experience without being explicitly programmed to do so. \n",
    "\n",
    "In the field of data science however, it is more helpful to think of machine learnining as a means of building models of data.\n",
    "\n",
    "Fundamentally, ML learning involves building mathematical models to help understand data. 'Learning' is when these models are given tunable paramenters that can be adapted to the observed data; in this way a program can be considered to be 'learning' from the data. Once these models have been 'fit' to previously seen data, they can then be used to predict and understand apsects of new unseen data. \n",
    "\n",
    "This article does a great job of explaining machine learning in laymans terms, all images in this notebook came from this site: \n",
    "\n",
    "https://vas3k.com/blog/machine_learning/\n",
    "\n",
    "---\n",
    "\n",
    "## The Machine Learning Process\n",
    "No matter what algorithm or model is used to solve a problem with ML, the overall process is mostly the same and generally follows these 7 steps:\n",
    "\n",
    "1. Define the Objective\n",
    "2. Data Gathering\n",
    "3. Prepring Data\n",
    "4. Data Exploration\n",
    "5. Building a Model\n",
    "6. Model Evaluation\n",
    "7. Predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Categories of Machine Learning\n",
    "\n",
    "<img src='data/images/ml1.png'>\n",
    "\n",
    "\n",
    "At the most fundamental level, ML can be categorized into Five broad categories:\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Reinforcemnt Learning\n",
    "4. Ensemble Methods\n",
    "5. Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Classical Machine Learning\n",
    "The first two main methods of machine learning (Supervised and Unsupervised) are often referred to as classical machine learning as they are well established and have been around since the 50's. Classic Machine Learning models tend work best with **simple data that has clear features**\n",
    "\n",
    "<img src='data/images/ml3.png'>\n",
    "\n",
    "---\n",
    "\n",
    "## 1 - Supervised Learning \n",
    "Supervised Learing involves modeling the relationship between measured 'features' of data and some 'label' (or desired outcome) associated with the data. There are two main types of Supervised Learning Classification and Regression.\n",
    "\n",
    "---\n",
    "\n",
    "### Important ML and Supervised Learning Definitions:\n",
    "\n",
    "**Algorithm** - A set of rules and statistical techniques used to learn paterns from data\n",
    "\n",
    "**Model** - A model is trained by using a machine learning algorithm\n",
    "\n",
    "**Predictor (aka Feature) Variables** - These are the independent (X) variables used in supervised analysis in order to predict an outcome variable\n",
    "\n",
    "**Label (aka Target or Outcome) Variables** - The dependent y variable that is the overall desired outcome of the ML analysis and is calculated using 1 or more feature variables\n",
    "\n",
    "**Training Data** - are the observations in the training data set form the experience that the algorithm uses to learn, therefore the training data effectively builds the ML model. \n",
    "\n",
    "**Testing Data** - are a set of observations used to evaluate the performance of the model using some performance metric. \n",
    "\n",
    "**Over-Fitting** - is when an overly complex model (too many features, etc.) predicts well with the training data, but does not work with new examples (testing set or real-world data), **Regularization** can be applied to many models to reduce over-fitting (see Lasso, Ridge, and Elastic-Net notebook for examples of this)\n",
    "\n",
    "**Under-Fitting** - occurs when a ML model is not complex enough to accurately capture relationships between a dataset's features and target variable. \n",
    "\n",
    "**Validation/Hold-Out Data** - in addition to the training and testing sets, sometimes a third set called a validation set can be required. These set is used to tune variables called **hyper parameters** which control how the model is learned. \n",
    "\n",
    "**Cross-Validation** - partitions the training data then trains the algorithm using all but one of the partitions which is used for the testing. The partitions are then rotated several times so that the algorithm is trained and evaluated on all of the data. (Example: Inital data with 4 subsets (A, B, C, D), Initial Partition (A, B, C, trained on D), Next rotation parition (B, C, D, trained on A), and so on. \n",
    "\n",
    "\n",
    "### ML Model Bias, Variance, and Error\n",
    "When a ML model (such as classification or regression) makes predictions from a set of data, the performance of the model can be described in terms of the prediction error on all examples not used to train the model, this is referred to as the **model error**\n",
    "\n",
    "**Model Error** can be decomposed into three sources of error, the **variance** of the model, the **bias** of the model, and the variance of the **irreducible error** in the data, therefore Model Error is equal to:\n",
    "* Error(Model) = Varaince(Model) + Bias(Model) + Variance(Irreducible Error)\n",
    "\n",
    "**Bias** is a measure of how close a model can predict a mapping function between inputs and outputs of a data set. A **bias error** occurs due to incorrect assumptions about the data such as assuming that data is linear when in reality it follows a complex function. \n",
    "* **Unbiased Models** - make weak or no assumptions about the form of the unknown underlying function that maps inputs to outputs in a dataset. Unbiased models treat all variables equally and become more complex as new variables are added, these models tend to have lower bias (fits the line well) and higher variance (unreliable predictions). Unbiased models are prone to overfitting (adding unessesary 'noise' to the analysis)\n",
    "\n",
    "* **Biased Models** - make strong assumptions about the form of the unknown underlying function that maps inputs to outputs in a dataset. These models will tend to have higher bias (doesn't fit the line as well) and lower variance (has consistent predictions), OLS linear regression is an example of one such model as these models are used to predict the best fit line formula (i.e. the unknown underlying function). Biased models are prone to underfitting \n",
    "\n",
    "\n",
    "**Variance** - is a measure of sensitivity of a model when it is fitted to new training data. \n",
    "* **Low Variance** models have small changes to the model with new traning data\n",
    "* **High Variance** models have large changes to the model with new training data\n",
    "\n",
    "**Irreducible Error** are errors that cannot be removed with any model (elements outside of control such as statistical noise)\n",
    "\n",
    "**bias-variance trade-off** is a useful conceptualization for selecting and configuring models, although generally cannot be computed directly as it requires full knowledge of the problem domain, which we do not have. Nevertheless, in some cases, we can estimate the error of a model and divide the error down into bias and variance components, which may provide insight into a given model’s behavior.\n",
    "\n",
    "The ideal ML algorithm has low bias (can accurately model the true relationship) and low variance (produces consistent predictions accross different datasets). In reality this is very challenging and is really the goal of applied machine learning for a given predictive modeling problem. \n",
    "\n",
    "The 'Trade-Off' is that reducing bias be achieved by increasing variance and conversely reducing variance can be achieved by incresing the bias. This relationship is referred toa s the bias-variance trade-off which is a conceptual framework for thinking about how to choose models and model configurations. \n",
    "\n",
    "For example, a model can be chosen based on its bias or variance. \n",
    "* **Simple models** such as linear regression and logistic regression, generally **have a high bias and a low variance**\n",
    "* **Complex models** such as random forest, generally **have a low bias but a high variance**\n",
    "\n",
    "Model configurations can be chosen based on their effect on the bias and variance of the model. For example, the k hyperparameter in k-nearest neighbors controls the bias-variance trade-off in that model. Small values, such as k=1, result in a low bias and a high variance, whereas large k values, such as k=21, result in a high bias and a low variance.\n",
    "\n",
    "High bias is not always bad, nor is high variance, but they can lead to poor results.\n",
    "\n",
    "We often must test a suite of different models and model configurations in order to discover what works best for a given dataset. A model with a large bias may be too rigid and underfit the problem. Conversely, a large variance may overfit the problem.\n",
    "\n",
    "We may decide to increase the bias or the variance as long as it decreases the overall estimate of model error.\n",
    "\n",
    "Technically, there is no one peformance metric that can calculate the bias-variance trade-off, because the true ideal 'mapping' function for a predictive modeling problem is unknown. In some cases the bias-variance trade off can be measured. \n",
    "\n",
    "See this link for more deatils on this: https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/\n",
    "\n",
    "\n",
    "The next section breaks down bias and variance visually:\n",
    "\n",
    "<img src='data/images/bias1.png' width=400px>\n",
    "\n",
    "The image below shows a differrent ML model that folows the true arc of the data relationship with a curvy line:\n",
    "\n",
    "<img src='data/images/bias2.png' width=400px>\n",
    "\n",
    "In order to compare how well both the straight and curvy lines fit the training data set by calculating their sums of squares. In other words, measure the distances from the fit lines to the data then add them up. \n",
    "\n",
    "<img src='data/images/bias3.png'>\n",
    "\n",
    "Note that the curvy line fits the data so well that the distances between the line and the data are all 0. So for the training data set, the curvy line outperforms the straight line. But remember, there is also a testing set:\n",
    "\n",
    "<img src='data/images/bias4.png'>\n",
    "\n",
    "If the sum of squares is calucated for the testing set, the straight line would outperform the curvy line:\n",
    "\n",
    "<img src='data/images/bias5.png'>\n",
    "\n",
    "So the lesson here is that even though the curvy line did a great job of fitting the training set, it did a horrible job of fitting the testing set.\n",
    "\n",
    "\n",
    "In the above example, the **complex (curvy line) model has a low bias** due to its flexibility and ability to adapt to the curve, yet it has **high variability** because it results in vastly different sums of squares for different datasets resulting in potentially widely unpredictable results between datasets. Because the curvy line fits the training set really well, but not the testing set, it is overfit.\n",
    "\n",
    "In contrast, the **simple (straight line) model has a high bias**, since it can not capture the curve in data relationship, yet it has **low variance** because the sum of squares are very similar for different datasets. Therefore, the straight line may only give good predictions (not great), but they will be consistent. \n",
    "\n",
    "\n",
    "\n",
    "The bottom line here is, even though a regression model might fit the regression line well (low bias), the model may be inacurate from a predictive point of view. \n",
    "\n",
    "\n",
    "Here is another link explaining the bias/variance concept: https://www.geeksforgeeks.org/bias-vs-variance-in-machine-learning/\n",
    "\n",
    "---\n",
    "\n",
    "### Classification \n",
    "Supervised learning models that attempt to identify which category an object belongs to and have discrete or categorical labels (yes/no, 1/0, red/blue, rain/no rain, ect.)\n",
    "\n",
    "**What is Classification used for?**\n",
    "* Filtering (spam, ect.)\n",
    "* Language Detection\n",
    "* Document Search\n",
    "* Sentiment Analysis\n",
    "* Handwriting Recognition\n",
    "* Fraud Detection\n",
    "\n",
    "**Popular Classification Algorithms**\n",
    "* Naive Bayes\n",
    "* Decistion Trees\n",
    "* Logistic Regression\n",
    "* K-Nearest Neighbors (KNN)\n",
    "* Support Vector Machines (SVM)\n",
    "\n",
    "---\n",
    "### Regression\n",
    "Supervised learning models that are used for estimating the relationships between a dependent ((y) label, outcome, or target variable) and one or more independent variables ((X) predictor, covariate, or feature variable). Regression models use continuous data and are used to forecast some numerical value.\n",
    "\n",
    "**What is Regression Used For?**\n",
    "* Stock Price Forecasts\n",
    "* Demand and Sales Volume Analysis\n",
    "* Medical Diagnosis\n",
    "* Any time-series correlations\n",
    "\n",
    "**Popular Regression Algorithms**\n",
    "* Linear Regression\n",
    "* Ridge/Lasso/Elastic-Net Regression\n",
    "* Polynomial Regression\n",
    "\n",
    "\n",
    "**Note: many ML models can be used with both regression and classification tasks**\n",
    " \n",
    "---\n",
    "\n",
    "## 2 - Unsupervised Learning\n",
    "Unsupervised learning involves modeling the features of a dataset without reference to any label (no pre-determined desired outcome values) There are two main types of unsupervised learning, Clustering and Dimensionality Reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### Clustering\n",
    "Clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. Basically clustering models attempt to divide objects into gropus based on unknown features. \n",
    "\n",
    "**What is Clustering Used For?**\n",
    "* Market segmentation\n",
    "* Merge close points on a map\n",
    "* Image compression\n",
    "* Analyze and label new data\n",
    "* Detect abnormal behavior\n",
    "\n",
    "**Popular Clustering Algorithms**\n",
    "* K-Means\n",
    "* Mean-Shift\n",
    "* Hierachical Clustering\n",
    "* DBSCAN\n",
    "\n",
    "---\n",
    "\n",
    "### Dimensionality Reduction\n",
    "Dimensionality Reduction is the transformation of data from a high-dimensional space into a low-dimensional space (e.g. reducing the number of random variables to consider) so that the low-dimensional representation retains some meaningful properties of the original data\n",
    "    \n",
    "**What is Dimensionality Reduction Used For?**\n",
    "* Recommendation systems\n",
    "* Beatiful visualizations\n",
    "* Topic m (and similar document search)\n",
    "* Fake image analysis\n",
    "* Risk management\n",
    "\n",
    "**Popular Dimensionality Reduction Algorithms**\n",
    "* Principle Component Analysis (PCA)\n",
    "* Singular Value Decomposition (SVD)\n",
    "* Latent Semantic Analysis (LSA, pLSA, GLSA)\n",
    "* Latent Dirichlet Allocation (LDA)\n",
    "* t-SNE (for visualization\n",
    " \n",
    "---\n",
    "\n",
    "## 3 - Reinforcement Learning\n",
    "Reinforcement learning is a type of ML where an agent learns to behave in an unknown environment by performing actions and learning from the results. There is no expected output in this type of learning like there is in supervised learning and no training data either as the ai agent learns as it goes from trail and error \n",
    "\n",
    "**What is Reinforcement Learning Used For?**\n",
    "* Self-driving cars\n",
    "* Robot vacuums\n",
    "* games\n",
    "* Automated trading\n",
    "* Enterprise resource management\n",
    "\n",
    "**Popular Reinforcement Learning Algorithms**\n",
    "* Q-Learning\n",
    "* SARSA\n",
    "* DQN\n",
    "* A3C\n",
    "* Genetic Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "## 4 - Ensemble Methods\n",
    "Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
    "\n",
    "**What are Ensemble Methods Used For?**\n",
    "* Everyting that fits classical ML algorithm approaches (ensemble mothos usually work better however)\n",
    "* Search systems\n",
    "* Computer vision\n",
    "* Object detection\n",
    "\n",
    "**Popular Reinforcement Learning Algorithms**\n",
    "* Random Forrest\n",
    "* Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "## 5 - Neural Networks and Deep Learning\n",
    "Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. **Neural Network Models can be supervised, semi-supervised or unsupervised**.\n",
    "\n",
    "**Neural Networks** are basically a collection of neurons (nodes) and the connections between them. A single neuron is a function with a number of inputs and one output and its task is to take all the numbers from its input/s, perform a function on them, and send the result to the output. \n",
    "\n",
    "**Simple Example of a single neuron in action:** sum up all numbers from the inputs and if that sum is bigger than N return 1, else return 0. \n",
    "\n",
    "**Single Layer Neural Network (Perceptron)** is the most simple form of neural network, in which there is only one layer of input nodes that send weighted inputs to a subsequent layer of receiving nodes, or in some cases, one receiving node. This single-layer design was part of the foundation for systems which have now become much more complex. \n",
    "\n",
    "**Multi-Layer Neural Network (Multilayer Perceptron)** contains more than one layer of artificial neurons or nodes. They differ widely in design. It is important to note that while single-layer neural networks were useful early in the evolution of AI, the vast majority of networks used today have a multi-layer model.\n",
    "\n",
    "In Multi-Layered Neural Networks, neurons are linked between layers but not interlinked within each layer. see the image below where each vertical line of circles represents a layer (input layer, hidden layers, output layer) and note that the nodes in each layer are separate from one another within, but connected to nodes in outside layers:\n",
    "\n",
    "<img src='data/images/ml4.png'>\n",
    "\n",
    "**What are Neural Networks and Deep Learning Used For?**\n",
    "* Replacement of all algorithms mentioned above\n",
    "* Photo and video object identification\n",
    "* Speech recognition and synthesis\n",
    "* Image processing, style transfer\n",
    "* Machine translation\n",
    "\n",
    "**Popular Reinforcement Learning Algorithms**\n",
    "* Perceptron\n",
    "* Convolutional Network (CNN)\n",
    "* Recurrent Networks (RNN)\n",
    "* Autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "## 6 - Model Evaluation\n",
    "In sklearn, there are 3 different APIs for evaluating the quality of a model’s predictions. \n",
    "\n",
    "* **Scoring Parameter** - are model evaluation tools (ex: cross-validation) that rely on an internal sklearn scoring strategy based on the convention that higher scores are better than lower scores, see link for more info:\n",
    "    * https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "* **Metric Functions**  - the metrics module implmements functions assessing prediction errors for specific purposes, usually on a model to model basis (i.e. regression metrics, classification metrics, Clustering meterics, and Multilabel ranking metrics)\n",
    "* **Estimator Score Method** - estimators have a score method providing a default evaluation criterion for the problem they are designed to solve. \n",
    "\n",
    "The link below explains the overall model evaluation process in detail:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7 - Improving model accuracy\n",
    "Here is the link that contains most of the info in this section:\n",
    "https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/\n",
    "\n",
    "There are many different methodologies used for improving model accuracy including:\n",
    "1. **Adding more data**\n",
    "2. **Treat missing values** - there are many ways to do this:\n",
    "    * **Deletion** - simply delete the entire sample with a missing value\n",
    "    * **Replacement** - replace the value with a sensible value (mean/median/mode for continuous variables)\n",
    "    * **Prediction** - create a predicitve model to handle missing data that will estimate the best value to put in the missing values place\n",
    "    * **KNN Imputation** - replaces (or imputes) the missing values of an attribute using the given number of attributes most similar to the attribute with missing values (good for classification)\n",
    "3. **Removing Unecessary Outliers*8\n",
    "4. **Feature Engineering** - is the process of extracting new features from existing data and/or features. There are two steps to feature engineering:\n",
    "    * **Feature Transformation** - there are 3 main scenarios where feature transformation can be required:\n",
    "        * **Normalization** is used to change the scale of the data to between 0 and 1 (example is having 3 variables of differing scale like meters, centimeters, and kilo-meters)\n",
    "        * **Remove Skewness** (for models that work well with normally distributed data) using log, square root, or inverse of the values\n",
    "        * **Create Bins** - good for dealing with outlier data that cannot be deleted, numeric data can also be made discrete this way\n",
    "    * **Feature Creation** - is when new variables are derived from existing variables, in many cases this will help to increase a models accuracy by bringing out hidden relationships between variables that may not have been obvious when in isolation\n",
    "5. **Feature Selection** is the process of finding the best subset of attributes which better or best explains the relationship between the indepenent variables and the target variable. \n",
    "6. **Using Multiple ML Algorithms** - in many cases, using multiple algorithms for a single problem can be useful for seeing which is best suited for a given problem\n",
    "7. **Alogirithm Parameter Tuning** - dialing in the optimum parameter values can greatly increase model performance\n",
    "8. **Ensemble Learning** - is the art of combining diverse set of learners (individual models) together to improvise on the stability and predictive power of the overall model (this technique is performed in many machine learning comptetions). There three commonly used ensemble learning techniques:\n",
    "    * **Bagging** - attempts to implement similar learners on small sample populations and then takes a mean of all the predictions, this method helps decrease variance errors\n",
    "    * **Boosting** - an iterative technique which adjusts the weight of an observation based on the last classification type. This method in general decreases the bias error (can sometimes lead to overffiting)\n",
    "    * **Stacking (Stacked Generalization)** - explores a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable of learning some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and use them to build an intermediate prediction (one prediction for each learned model) then add a new model which learns from the intermediate predictions the same target. This final model is said to be stacked on the top of the others, hence the name. Thus, you might improve your overall performance, and often you end up with a model which is better than any individual intermediate model.\n",
    "\n",
    "### Note that all of the above methods can improve the accuracy of a model, but note that, HIGHER ACCURACY MODELS DO NOT ALWAYS PERFORM BETTER FOR NEW UNSEEN DATA POINTS. Overffiting can also cause misleading improvement readings in a models accruracy. In order to avoid this pitfall cross validation must be used\n",
    "\n",
    "9. **Cross-Validation** - one of the most important concepts in data modeling. There are two broad types of cross-validation:\n",
    "\n",
    "**Non-Exhaustive CV Methods**\n",
    "\n",
    "Non-exhustive cv methods do not compute all the possible ways of splitting up original data. Below are a few non-exhaustive cv methods:\n",
    "\n",
    "\n",
    "* **Holdout Method** - the most basic example, thie approach divides the entire dataset into two parts (training and testing). Usually the training set is more than twice the size of the testing data, usually a ratio of 70:30 or 80:20. \n",
    "    * In this approach the data is shuffled randomly and the model is trained on a different combination of data points so the model can give different results every time it is trained (which can cause instability). In addition, one can never be assured that the random training set is truly representative of the data set as a whole. This method can be good to use with a very large dataset, if your in a hurry, or just as an initial starting point to feel a model out. \n",
    "* **K-Fold Cross-Validation** - this method guarantees that the score of the model will not depend on the way the training and test sets are picked, the process for this method is as follows:\n",
    "     1. Randomly split entire dataset into k number of subsets (or folds)\n",
    "     2. For each subset, build the model on k-1 subsets of the dataset, then test the data on the kth fold (the one held out)\n",
    "     3. This process is repeated until each of the subsets has been used as the test set\n",
    "     4. the average of the each iterations scores is called the cross-validation accuracy score and serves as the performance metric for the model\n",
    "      5. Because k-fold cv ensures that every observation from the original dataset has the chance of appearing in a training and test set, it usually results in a less biased model and is one of the best approaches if there is **limited input data**. The main disadvantage is that it can be really slow to run\n",
    "* **Stratified K-Fold Cross-Validation** - because the above k-fold cv uses random data selection for subsets, in many cases there can be highly imbalanced folds which can create traning bias. Stratification rearranges the data to ensure that each fold is a good representation of the whole. \n",
    "\n",
    "**Exhaustive Methods**\n",
    "\n",
    "Exhaustive cv methods test on all of the possible ways to divide the original data sample into a training/testing validation set. Below are a few exhuastive cv methods:\n",
    "\n",
    "* **Leave-P-Out Cross-Validation**\n",
    "* **Leave-One-Out Cross-Validation**\n",
    "\n",
    "**Note that usually either cross-validation is used or test/train/split, but not both.**\n",
    "\n",
    "Below is a link to a good article on cross-validation:\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Choosing an Algorithm\n",
    "It is imperative to choose an algorithm that best fits the data and desired predictions. Below is a diagram that helps with this process:\n",
    "\n",
    "<img src='data/images/choose.png'>\n",
    "<img src='data/images/choose2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
