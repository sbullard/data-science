{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Artificial Intelligence with Python\n",
    "\n",
    "## Part V - Learning\n",
    "\n",
    "Harvard CS50 Introduction to Artificial Intelligence with Python is an online course that I took in the Spring of 2020. It consisted of 6 lectures of which I have a notebook for each. Each lecture had 2 projects, those are located in the projects folder in the same directory as this notebook.\n",
    "\n",
    "[Course Link](https://cs50.harvard.edu/ai/)\n",
    "\n",
    "[Lecture Link](https://www.youtube.com/watch?v=E4M_IQG0d9g&list=PLhQjrBD2T382Nz7z1AEXmioc27axa19Kv&index=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Machine Learning\n",
    "Machine Learning (ML) is the study of computer algorithms that improve automatically through experience. ML algorithms build a mathematical model based on sample data (also called 'training' data) in order to make predictions or decisions without being explicitly told to do so. \n",
    "\n",
    "---\n",
    "## Supervised Learning\n",
    "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. There a number of supervised machine learnining tasks or types:\n",
    "\n",
    "### Classification (Supervised ML Task)\n",
    "task of learning a function mapping an input point to a discrete category (example, red or blue, rain or not rain, authentic or counterfit)\n",
    "* **nearest-neighbor classification** - a classifiation algorithm that, given an input, chooses the class (or category) of the nearest data point to that input\n",
    "\n",
    "\n",
    "* **k-nearest-neighbor classification** - a classification algorithm that, given an input, chosses teh most common class out of the k nearest data point to that input\n",
    "    \n",
    "**Classification example:**\n",
    "* inputs for ml algorithm: x1 = humidity, x2 = pressure\n",
    "* given the two above inputs, use hypothesis function h(x1, x2) in order to predict weather. \n",
    "* linear combinations (or weights) are assigned to each input in side of the function like: h(x1, x2) = Rain if w0 + w1*x1 + w2*x2 >= 0 else NO Rain\n",
    "* In real practice Rain and No rain would be represented by numbers like 1 for rain an 0 for No Rain: 1 if w0 + w1*x1 + w2*x2 >= 0, else 0\n",
    "\n",
    "\n",
    "* **Vectors** are often used for various value sequences, i.e. a vector for all the weights and a vector for all the inputs\n",
    "\n",
    "\n",
    "* **Weight Vector** - vector to hold all assigned weights for a given problem: W: (w9, w1, w2)\n",
    "\n",
    "\n",
    "* **Input Vector** - vector to hold all problem inputs X: (1, x1, x2)\n",
    "\n",
    "The assigned weights above are ultimately the slope of the end result and optimal weight choice is key for more accurate solutions. The ML algorithms determine the ideal weights\n",
    "\n",
    "* **Dot Product** - the dot product of two vectors takes each matching element between the two vectors and multiplies them together, then adds then sums all the resulting products. (Note that both vectors must be the same length for this to work)\n",
    "    * Dot Product of W and X: w0*1 + w1*x1 + w2*x2\n",
    "  \n",
    "    \n",
    "An adjusted hypothesis formula (parameterized by weight) for the above rain example using dot product would be:\n",
    "* $ h_w(X) $ = 1 if W dot X >= 0, else 0 \n",
    "\n",
    "---\n",
    "### Perceptron (Supervised ML Algorithm)\n",
    "The perceptrion is an algorithm for supervised learning of binary classifiers using linear regression\n",
    "\n",
    "* **linear classifier** - a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector\n",
    "\n",
    "\n",
    "* **binary classifier** - A binary classifier is a function which can decide whether or not an input (represented by a vector) beolongs to some specific class. Binary classifiers are a type of linear classifier\n",
    "\n",
    "**Perceptron Learning Rule Example:**\n",
    "\n",
    "* given data point (x,y), update each weight according to:\n",
    "    * $ w_i = w_i + \\alpha(actual value - hypothesis) * x_i$ \n",
    "\n",
    "The general idea of the above formula is to take random starting weights, apply them to test (hypothesis) functions and learn from them to adjust the weights accordingly\n",
    "\n",
    "Using the above formula, a correct hypothesis would equal the actual value and the result would just be the original weights. However, if the hypothesis is not correct, then weights may need to be adjusted.\n",
    "* If the actual value > than hypothesis, weights need to increase\n",
    "* If the actual value < than hypothesis, weights need to decrease\n",
    "* Note that $\\alpha$ in the above formula represents the learning rate, which is the factor that weights will be adjusted. If alpha is larger then weights adjust more and vice-versa. \n",
    "\n",
    "\n",
    "**Hard Threshold**\n",
    "the above analysis we've performed on the whether will return only two possibilites, Rain and No Rain based on a threshold set by the ML algorithm. This is then is a hard threshold as decisions are made based on set results (above 1 for Rain, 0 for no rain). There is no level of assuredness to the results (i.e. 80% sure it will rain). \n",
    "\n",
    "<img src ='data/hard_threshold.png'>\n",
    "\n",
    "Note that the above graph is for the weather example and that it only has 0 and 1 plot points. \n",
    "\n",
    "\n",
    "**Soft Threshold**\n",
    "Logistic functions can be used to create 'soft thresholds' or more accurate results predicting the probability of an event with real number outputs in between 0 and 1 rather than just 0 and 1.\n",
    "\n",
    "<img src ='data/soft_threshold.png'>\n",
    "\n",
    "---\n",
    "### Support Vector Machines (Supervised ML Algorithms)\n",
    "Supervised ML models with associated learning algorithms that analyze data used for classification and regression analysis. SVM's are usually used used to locate the maximum margin separator between two data sets. SVM's are useful for data without clear linear separation between the data sets.\n",
    "\n",
    "**Maximum Margin Separator** - boundry that maximizes the distance between any of the data points\n",
    "\n",
    "In the image below, the line represents the max margin separator bewteen the blue and red sections.\n",
    "\n",
    "<img src='data/max_margin_separator.png'>\n",
    "\n",
    "The above image is contrived and SVM's are more useful for data sets like this one:\n",
    "\n",
    "<img src='data/svm.png'>\n",
    "\n",
    "In such a case as above, SVM's can operate in higher dimensions and return a line of sepration like this:\n",
    "\n",
    "<img src='data/svm2.png'>\n",
    "\n",
    "---\n",
    "### Regression (Supervised ML Task )\n",
    "Supervised learning task of learnign a function mapping an input point to a continuous value, often used in business (example, company trying to predict sales numbers based on amount spent on advertising)\n",
    "\n",
    "* f(advertising)\n",
    "* f(1200) = 5800 <--- 1200 spent on advertising, results 5800 in sales\n",
    "\n",
    "Taking past advertising data like above, a hypothesis function can\n",
    "* h(advertising)\n",
    "\n",
    "<img src='data/lin_reg1.png'>\n",
    "\n",
    "In the above case, the linear regression line is not measuring separation between sections like before, but rather the predicted sales results based on amount spent on advertising. \n",
    "\n",
    "\n",
    "### Evaluating Hypothesis\n",
    "In order to evaluate the results of a hypothesis functions, optimization methodologies can be used. Recall that with optimization, cost functions were used. In learning algorithms, loss functions can be used in a similar manner.\n",
    "\n",
    "**Loss Function** - a function that expresses how poorly our hypothesis performs\n",
    "\n",
    "**0 - 1 loss function** - used for discrete categories (red or blue, rain or no rain), goal is to minimize the loss\n",
    "* the 0-1 function takes actual results (rain, no rain) and the prediction (rain, no rain) as inputs and looks like\n",
    "* LF(actual, predicted) = 0 if actual equals prediction, else 1\n",
    "\n",
    "If a data point maches the prediction, then it is assigned a value of 0 and no loss occurs, however, if it misses a prediction then a value of 1 is assigned and loss occurs. All losses would be summed together for the total loss, 4 in the image below: \n",
    "\n",
    "<img src='data/loss_func.png'>\n",
    "\n",
    "The loss number can then be used to rate the results. If a different linear regression line is used that decreases the number of losses then that would be preferable. The goal is to try and minimze the loss amount.\n",
    "\n",
    "\n",
    "**L1 Loss Function** - used for continous values rather than discrete (company example) which takes into account both how close the predition was to actual values and how far apart they ultimately were. For example in the case of the advertising budget to sales results, we want the prediction to match the actual results, but if not we care how close our prediction came to results. \n",
    "* L1(actual, predicted) = abs(actual - predicted)\n",
    "\n",
    "In the image below, the regression line is the prediction and all the smaller lines represent the L1 loss for each data point. All of the L1 loss must be summed together to get total loss.\n",
    "\n",
    "<img src='data/l1_loss.png'>\n",
    "\n",
    "\n",
    "**L2 Loss Function** - similar to L1 loss function but uses the square rather than the absolute value of teh total loss differences. Because of the squaring, this function penalizes values further away from the prediction line (translation, not good if multiple outliers present in data, use L1 instead)\n",
    "* L2(actual, predicted) = (actual - predicted)^2\n",
    "\n",
    "\n",
    "All of the above loss functions can fall prey to overfitting\n",
    "\n",
    "**Overfitting** - a model that fits too closely (or exactly) to a particular data set and therefore may fail to generalize to future data\n",
    "\n",
    "Taking the lineare line regression example before, overfitting the data to account for even the most egregious outliers could result in something like this: \n",
    "\n",
    "<img src='data/ofit1.png'>\n",
    "\n",
    "and the L1 loss example of overfitting could be a line generated that creates zero loss but does not generalize well at all like this:\n",
    "\n",
    "<img src='data/ofit2.png'>\n",
    "\n",
    "\n",
    "**Regularization** - penalizing hypotheses that are more complex to favor simpler, more general hypothesis (this process helps prevent overfitting) a function for regularization could be:\n",
    "* cost(h) = loss(h) + $\\lambda$ * complexity(h)\n",
    "\n",
    "In the function above, complexity is some measure of the complexity of the hypothesis that is user defined and is related to the complexity of the expected regression line data and outcome. The idea is to give preference to simpler results (straight line) over more complex (overfitting examples), there needs to be a balance between the loss and the complexity, and this is represented by the lambda $\\lambda$ and is a value chosen to scale complexity. \n",
    "\n",
    "\n",
    "**Holdout Cross-Validation** - splitting data into a training set and a test set, such taht learning happens on the training set and is evaluated on the test set\n",
    "\n",
    "**K-Fold Cross-Validation** - splitting data into k sets, and experimenting k times, using each set as a test set once, and using the remaining data as the training \n",
    "\n",
    "\n",
    "## Scikit-Learn\n",
    "Is a library that allows for quick and easy usage of many machine learning alogrithms and models. It already contains implementations for many of the algorithms already discussed. \n",
    "\n",
    "\n",
    "the following examples in code use a data set of banknotes that contains 4 different input values for each data point (individual banknote), there is also a label for each banknote with a 1 or a 0 where 1 means counterfit and 0 means non-counterfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.895690</td>\n",
       "      <td>3.00250</td>\n",
       "      <td>-3.606700</td>\n",
       "      <td>-3.44570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.476900</td>\n",
       "      <td>-0.15314</td>\n",
       "      <td>2.530000</td>\n",
       "      <td>2.44950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.910200</td>\n",
       "      <td>6.06500</td>\n",
       "      <td>-2.453400</td>\n",
       "      <td>-0.68234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607310</td>\n",
       "      <td>3.95440</td>\n",
       "      <td>-4.772000</td>\n",
       "      <td>-4.48530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.371800</td>\n",
       "      <td>7.49080</td>\n",
       "      <td>0.015989</td>\n",
       "      <td>-1.74140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>-2.967200</td>\n",
       "      <td>-13.28690</td>\n",
       "      <td>13.472700</td>\n",
       "      <td>-2.62710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>0.318030</td>\n",
       "      <td>-0.99326</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.88619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-0.025314</td>\n",
       "      <td>-0.17383</td>\n",
       "      <td>-0.113390</td>\n",
       "      <td>1.21980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-2.234000</td>\n",
       "      <td>-7.03140</td>\n",
       "      <td>7.493600</td>\n",
       "      <td>0.61334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>4.728500</td>\n",
       "      <td>2.10650</td>\n",
       "      <td>-0.283050</td>\n",
       "      <td>1.56250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness   curtosis  entropy  class\n",
       "0    -0.895690   3.00250  -3.606700 -3.44570      1\n",
       "1     3.476900  -0.15314   2.530000  2.44950      0\n",
       "2     3.910200   6.06500  -2.453400 -0.68234      0\n",
       "3     0.607310   3.95440  -4.772000 -4.48530      1\n",
       "4     2.371800   7.49080   0.015989 -1.74140      0\n",
       "...        ...       ...        ...      ...    ...\n",
       "1367 -2.967200 -13.28690  13.472700 -2.62710      1\n",
       "1368  0.318030  -0.99326   1.094700  0.88619      1\n",
       "1369 -0.025314  -0.17383  -0.113390  1.21980      1\n",
       "1370 -2.234000  -7.03140   7.493600  0.61334      1\n",
       "1371  4.728500   2.10650  -0.283050  1.56250      0\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# table rows represent an individual bank note\n",
    "# variance, skewness, curtosis, and entropy are the input values\n",
    "# class is the label, 1 = counterfeit note, 0 = non-counterfeit\n",
    "\n",
    "df = pd.read_csv(\"banknotes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model Perceptron\n",
      "Correct: 674\n",
      "Incorrect: 12\n",
      "Accuracy: 98.25%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn import svm   # support vector machine model\n",
    "from sklearn.linear_model import Perceptron  # Perceptron model\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neighbors import KNeighborsClassifier # Nearest neighbor\n",
    "\n",
    "# Swap Models as Necessary\n",
    "model = Perceptron()\n",
    "# model = svm.SVC()\n",
    "#model = KNeighborsClassifier(n_neighbors=1)\n",
    "# model = GaussianNB()\n",
    "\n",
    "# Read data in from file\n",
    "with open(\"banknotes.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append({\n",
    "            # first four input values seprated out into evidence list\n",
    "            \"evidence\": [float(cell) for cell in row[:4]],\n",
    "            # label column set as authentic or counterfeit\n",
    "            \"label\": \"Authentic\" if row[4] == \"0\" else \"Counterfeit\"\n",
    "        })\n",
    "\n",
    "# Separate data into training and testing groups\n",
    "holdout = int(0.50 * len(data)) # 50% of data removed for testing\n",
    "random.shuffle(data)\n",
    "testing = data[:holdout]        # half of data used for testing\n",
    "training = data[holdout:]       # half of data used for training\n",
    "\n",
    "# Train model on training set\n",
    "X_training = [row[\"evidence\"] for row in training] # the inputs\n",
    "y_training = [row[\"label\"] for row in training]    # the labels\n",
    "model.fit(X_training, y_training)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "X_testing = [row[\"evidence\"] for row in testing] # actual evidence\n",
    "y_testing = [row[\"label\"] for row in testing]    # actual values \n",
    "predictions = model.predict(X_testing)           # create predictions\n",
    "\n",
    "# Compute how well we performed\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "total = 0\n",
    "for actual, predicted in zip(y_testing, predictions):\n",
    "    total += 1\n",
    "    if actual == predicted:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "\n",
    "# Print results\n",
    "print(f\"Results for model {type(model).__name__}\")\n",
    "print(f\"Correct: {correct}\")\n",
    "print(f\"Incorrect: {incorrect}\")\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above code was mostly done manually, but scikit-learn has functions available that handle most of the dirty work, see below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model Perceptron\n",
      "Correct: 543\n",
      "Incorrect: 6\n",
      "Accuracy: 98.91%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = Perceptron()\n",
    "# model = svm.SVC()\n",
    "# model = KNeighborsClassifier(n_neighbors=1)\n",
    "# model = GaussianNB()\n",
    "\n",
    "# Read data in from file\n",
    "with open(\"banknotes.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append({\n",
    "            \"evidence\": [float(cell) for cell in row[:4]],\n",
    "            \"label\": \"Authentic\" if row[4] == \"0\" else \"Counterfeit\"\n",
    "        })\n",
    "\n",
    "# Separate data into training and testing groups\n",
    "evidence = [row[\"evidence\"] for row in data]\n",
    "labels = [row[\"label\"] for row in data]\n",
    "\n",
    "# Automatically split data into testing and training group with\n",
    "# train_test_split()\n",
    "X_training, X_testing, y_training, y_testing = train_test_split(\n",
    "    evidence, labels, test_size=0.4\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_training, y_training)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = model.predict(X_testing)\n",
    "\n",
    "# Compute how well we performed\n",
    "correct = (y_testing == predictions).sum()\n",
    "incorrect = (y_testing != predictions).sum()\n",
    "total = len(predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Results for model {type(model).__name__}\")\n",
    "print(f\"Correct: {correct}\")\n",
    "print(f\"Incorrect: {incorrect}\")\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reinforcement Learning\n",
    "Given a set of rewards or punishments, learn what actions to take in the future. The general idea is that the ai agent is an environemnt that rewards it and punishes it. Initially the environemnt puts the ai in some state (i.e. a game that the ai is playing  or a grid that the agent is exploring). In the state, the ai has to choose an action and then will get a new state and some sort of numerical reward (positive for good, negative for bad).\n",
    " \n",
    "The above world with an environemnt and ai agent being rewarded and punished is often represented with a Markow Decision Process\n",
    "\n",
    "**Markov Decision Process** - model for decision-making, representing states, actions, and their rewards\n",
    "* Begins with a set of states S\n",
    "* Begins witha  set of avaliable ACTIONS(s)\n",
    "* Transition Model P(s' | s, a)  <---given curernt state s and taking action a, what is the probability of s'\n",
    "* Reward function R(s, a, s') <--- what is the reward for being in state s, taking action a, and getting to state s'\n",
    "\n",
    "\n",
    "### Q-Learning (Reinforcement Learning Model) Overview\n",
    "method for learning a function Q(s, a), estimate of the value of performing action a in state s\n",
    "* start with Q(s,a) = 0 for all s, a\n",
    "* When the agent takes an action and receives a reward\n",
    "    * estimate the value of Q(s,a) based on current reward and expected future rewards\n",
    "    * update Q(s,a) to take into account old estimate as well as our new estimate\n",
    "    \n",
    "### Q-Learning in Practice (1:26 minutes into lecture)\n",
    "* start with Q(s,a)=0 or all s,a\n",
    "* every time we take an action a in state s and observe a reward r, we update: \n",
    "    * newQ(s,a) <-- oldQ(s, a) + $\\alpha$(new value estimate - old value estimate)\n",
    "    * $\\alpha$ represents how much we value new information over old information, 1 means highly value new information (only new estimate considered), 0 means only consider old estimate\n",
    "* newQ(s,a) <-- oldQ(s, a) + $\\alpha$(r + future reward estimate - oldQ(s,a)), where r=reward\n",
    "* newQ(s,a) <-- oldQ(s, a) + $\\alpha$(r + maxa'*Q(s',a'))- oldQ(s,a)\n",
    "\n",
    "Once the above Q-Learning is implemented and an ai starts to learn which estimates produce which rewards or punishments, then various other models can be used like:\n",
    "\n",
    "**Greedy Decision-Making**\n",
    "* When in state s, choose a with highest Q(s,a) <- highest estimated value, as with greedy best first search, this methodology is not always the most efficient\n",
    " \n",
    "**$\\epsilon$ - greedy** - a greedy decsision making algorithm\n",
    "* set epsilon equal to how often we want to move randomly\n",
    "* with probability 1 - $\\epsilon$, choose estimated best move\n",
    "* with probabilty $\\epsilon$, choose a random move\n",
    "\n",
    "One common application of reinforcement learning is used with having an ai learn how to play a game by letting it play the game a large number of times and figure out the reward signals by winning or losing. \n",
    "\n",
    "### Nim (1hr 33 min in vid)\n",
    "\n",
    "\n",
    "**Function Approximation** - approximating Q(s,a), often by a function combining various features, rather than storing one value for every state-action pair \n",
    "\n",
    "\n",
    "---\n",
    "## Unsupervised Learning\n",
    "given input data without any additional feedback, learns patterns\n",
    "\n",
    "**Clustering** - organizing a set of objects into group s in such a way that similar objects tend to be in the same groups, examples (genetic research, image segmentation, market research, medical imaging, social network analysis)\n",
    "\n",
    "**k-Means Clustering**\n",
    "Algorithm for clustering data based on repeatdely assigning points to clusters and updating those clusters' centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
